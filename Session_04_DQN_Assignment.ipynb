{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 04 DQN - Assignment\n",
    "\n",
    "\n",
    "In deep Q-learning, we use a neural network to approximate the Q-value function. The state is given as an input to the neural network. \n",
    "The output of the neural network represents the (estimated) Q-values of all possible actions. Using an argmax, we choose the action corresponding to the highest Q-value.\n",
    "\n",
    "\n",
    "To train the Q network, we sample a batch of stored experiences from the replay memory. An experience is a tuple of (state, action, reward, next_state).\n",
    "We input the state into the Q network and get the estimated Q-values. For the Q network to adjust the weights, it needs to have an idea of how accurate these predicted Q-values are.\n",
    "However, we do not know the target or actual value here as we are dealing with a reinforcement learning problem. The solution is to estimate the target value by using a second neural network, called the target network. This target network will take the next state as an input and predict the Q-values for all possible actions from that state. \n",
    "Now we can compute the labels $y$ to train the policy network: $y = R(s, a) + \\gamma max_{a'}Q(s', a') - Q_{t-1}(s, a)$.\n",
    "\n",
    "The Q network can now be trained with the MSE loss. It's important to know that the target network is an exact copy of the policy network and the weights of the target network \n",
    "\n",
    "After a certain amount of Q-network updates, we copy its weights to the target network.\n",
    "\n",
    "For more detailed information: https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np   \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Tensorflow libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.api.models import Sequential, load_model\n",
    "from keras.api.layers import Activation, Dense, Dropout, BatchNormalization, InputLayer\n",
    "from keras.api.optimizers import Adam\n",
    "from keras.api.losses import MeanSquaredError\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import threading\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus: tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDQNAgent:\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_size,\n",
    "            action_size,\n",
    "            learning_rate = 0.001,\n",
    "            epsilon = 1):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        initial_model = self._build_model()\n",
    "\n",
    "        self.q_network = initial_model\n",
    "        self.target_network = initial_model\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss=MeanSquaredError())\n",
    "        return model\n",
    "    \n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randint(self.action_size)\n",
    "        else:\n",
    "            q_values = self.q_network.predict(state, verbose=None)\n",
    "            return np.argmax(q_values)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDQNAgent:\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_size,\n",
    "            action_size,\n",
    "            memory_minlen=250,\n",
    "            memory_maxlen=2000,\n",
    "            gamma=0.95,\n",
    "            epsilon=1.0,\n",
    "            epsilon_min=0.01,\n",
    "            epsilon_decay=0.999,\n",
    "            model: Sequential | None = None,\n",
    "            target_update_freq=10\n",
    "        ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory_minlen = memory_minlen\n",
    "        self.memory = deque(maxlen=memory_maxlen)\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.model = model if model else self._build_model()  # Q-Network\n",
    "        self.target_model = self._build_model()  # Target-Network\n",
    "        self.update_target_network()  # Initialize target model weights\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.train_steps = 0  # To track when to update the target network\n",
    "        self.lock = threading.Lock()  # Ensure thread safety for model updates\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(optimizer=Adam(0.0003), loss=MeanSquaredError())\n",
    "        return model\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update the target network weights to match the Q-network.\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size)  # Random action\n",
    "        q_values = self.model.predict(state, verbose=None)\n",
    "        return np.argmax(q_values[0])  # Best action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < self.memory_minlen:\n",
    "            return  # Wait for enough samples\n",
    "        minibatch = random.sample(self.memory, self.memory_minlen)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                # Use the target network for stable Q-value estimation\n",
    "                target = reward + self.gamma * np.amax(self.target_model.predict(next_state, verbose=None)[0])\n",
    "            target_f = self.model.predict(state, verbose=None)\n",
    "            target_f[0][action] = target\n",
    "            with self.lock:  # Lock during model training\n",
    "                self.model.fit(state, target_f, batch_size=batch_size, epochs=1, verbose=None)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        # Update target network periodically\n",
    "        self.train_steps += 1\n",
    "        if self.train_steps % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MountainCar-V0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A car is on a one-dimensional track, positioned between two \"mountains\". The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n",
    "The agent (a car) is started at the bottom of a valley. For any given state the agent may choose to accelerate to the left, right or cease any acceleration.\n",
    "\n",
    "<img src=\"./NotebookImages/MountainCart.gif\">\n",
    "\n",
    "For a description of the statevector, the action space and the episode termination,have a look at:https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py\n",
    "\n",
    "- Implement a DQN to solve this environment.\n",
    "- Try to minimize the total number of steps per episode needed to reach the flag.\n",
    "- You are allowed to tweak the reward function. For example, giving an extra reward for getting closer to the flag.\n",
    "- Modify the DQN implementation into a deep SARSA implementation. Compare the deep SARSA to the DQN implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 0\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "#loaded_model = load_model(\"saved_models/mountain_car_model_v1.keras\")\n",
    "\n",
    "agent = CustomDQNAgent(state_size, action_size)\n",
    "\n",
    "def training_thread_fn():\n",
    "    batch_size = 32\n",
    "    while True:\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "training_thread = threading.Thread(target=training_thread_fn, daemon=True)\n",
    "training_thread.start()\n",
    "\n",
    "num_episodes = 1000\n",
    "try:\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        previous_x_position = state[0]\n",
    "\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        total_reward = 0\n",
    "        episode_length = 0\n",
    "\n",
    "        for t in range(1000):\n",
    "            action = agent.act(state)\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            if done:\n",
    "                reward = max(100, 300 - episode_length)\n",
    "            else:\n",
    "                velocity = abs(next_state[1])\n",
    "                if velocity < 0.009:\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    position_shift = (next_state[0] - previous_x_position)\n",
    "                    reward = (abs(position_shift) + velocity)\n",
    "\n",
    "            total_reward += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            previous_x_position = next_state[0]\n",
    "\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        print(f\"episode: {episode}, episode length: {episode_length}, reward: {total_reward}, epsilon: {agent.epsilon}\")\n",
    "finally:\n",
    "    env.close()\n",
    "    agent.model.save(\"saved_models/mountain_car_model_v1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 0\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "loaded_model_name = \"mountain_car_model_v1\"\n",
    "\n",
    "agent = CustomDQNAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    epsilon=0.00,\n",
    "    epsilon_min=0.00,\n",
    "    model=load_model(f\"saved_models/{loaded_model_name}.keras\"))\n",
    "\n",
    "num_episodes = 100\n",
    "try:\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        done = False\n",
    "        episode_length = 0\n",
    "        totalreward = 0\n",
    "\n",
    "        for t in range(1000):\n",
    "\n",
    "            action = agent.act(state)\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            totalreward += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            state = next_state\n",
    "        \n",
    "        print(f\"episode: {episode}, episode length: {episode_length}, reward: {total_reward}\")\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LunarLander-v2\n",
    "\n",
    "Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\n",
    "For more information abou this environment see: https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
    "\n",
    "<img src=\"./NotebookImages/LunarLander.gif\">\n",
    "\n",
    "- Implement a DQN to solve this environment. LunarLander-v2 defines \"solving\" as getting average reward of 200 over 100 consecutive trials. \n",
    "- Try to minimize the number of episodes it takes to solve the environment.\n",
    "- How would you tweak the reward function for the LunarLander to make a quicker descent.\n",
    "- Modify the DQN implementation into a deep SARSA implementation. Compare the deep SARSA to the DQN implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roel/.conda/envs/gym-tf-gpu/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1, episode length: 57, reward: -136.359107978066, epsilon: 0.3\n",
      "episode: 2, episode length: 79, reward: -134.4467858381095, epsilon: 0.3\n",
      "episode: 3, episode length: 65, reward: -141.9631122977795, epsilon: 0.3\n",
      "episode: 4, episode length: 73, reward: -97.79481893157897, epsilon: 0.3\n",
      "episode: 5, episode length: 79, reward: -153.9429480572283, epsilon: 0.3\n",
      "episode: 6, episode length: 94, reward: -213.48779438578987, epsilon: 0.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[1;32m     49\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[0;32m---> 51\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     53\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     54\u001b[0m     episode_length \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/gym-tf-gpu/lib/python3.12/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/.conda/envs/gym-tf-gpu/lib/python3.12/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/.conda/envs/gym-tf-gpu/lib/python3.12/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/.conda/envs/gym-tf-gpu/lib/python3.12/site-packages/gymnasium/envs/box2d/lunar_lander.py:675\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    672\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/.conda/envs/gym-tf-gpu/lib/python3.12/site-packages/gymnasium/envs/box2d/lunar_lander.py:786\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m--> 786\u001b[0m pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    788\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make('LunarLander-v2')#, render_mode=\"human\")\n",
    "#env.metadata[\"render_fps\"] = 0\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "loaded_model = load_model(\"saved_models/lunar_lander_model_v1.keras\")\n",
    "\n",
    "agent = CustomDQNAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    memory_minlen=32,\n",
    "    memory_maxlen=5000,\n",
    "    gamma=0.99,\n",
    "    epsilon=1,\n",
    "    epsilon_decay=0.999,\n",
    "    epsilon_min=0.01,\n",
    "    model=None,\n",
    "    target_update_freq=10\n",
    ")\n",
    "\n",
    "def training_thread_fn():\n",
    "    batch_size = 32\n",
    "    while True:\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "training_thread = threading.Thread(target=training_thread_fn, daemon=True)\n",
    "training_thread.start()\n",
    "\n",
    "reward_history_maxlen = 100\n",
    "\n",
    "reward_history = deque(maxlen=reward_history_maxlen) # last 100 episodes\n",
    "\n",
    "num_episodes = 1000\n",
    "episode = 0\n",
    "try:\n",
    "    for episode in range(num_episodes):\n",
    "        episode += 1\n",
    "        state, _ = env.reset()\n",
    "        previous_x_position = state[0]\n",
    "\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        total_reward = 0\n",
    "        episode_length = 0\n",
    "\n",
    "        for t in range(500):\n",
    "            action = agent.act(state)\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        reward_history.append(total_reward)\n",
    "\n",
    "        average_reward_recent_history = sum(reward_history)/reward_history_maxlen\n",
    "        \n",
    "        print(f\"episode: {episode}, episode length: {episode_length}, reward: {total_reward}, epsilon: {agent.epsilon}\")\n",
    "\n",
    "        if (average_reward_recent_history >= 200): # if average of reward history > 200 end\n",
    "            print(f\"on episode {episode}, the average reward on the last {reward_history_maxlen} episodes, exceeded 200 with: {average_reward_recent_history}\")\n",
    "            break\n",
    "\n",
    "        #agent.replay(batch_size=32)\n",
    "finally:\n",
    "    env.close()\n",
    "    agent.model.save(\"saved_models/lunar_lander_model_v1.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode=\"human\")\n",
    "env.metadata[\"render_fps\"] = 60\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "loaded_model_name = \"lunar_lander_model_v1\"\n",
    "\n",
    "agent = CustomDQNAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    epsilon=0.00,\n",
    "    epsilon_min=0.05,\n",
    "    model=load_model(f\"saved_models/{loaded_model_name}.keras\"))\n",
    "\n",
    "num_episodes = 100\n",
    "try:\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        done = False\n",
    "        episode_length = 0\n",
    "        totalreward = 0\n",
    "\n",
    "        for t in range(500):\n",
    "\n",
    "            action = agent.act(state)\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            totalreward += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            state = next_state\n",
    "        \n",
    "        print(f\"episode: {episode}, episode length: {episode_length}, reward: {total_reward}\")\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL: CarRacing-v0\n",
    "\n",
    "\n",
    "Description of the environment:\n",
    "\n",
    "Easiest continuous control task to learn from pixels, a top-down racing environment. Discreet control is reasonable in this environment as well, on/off discretisation is fine. State consists of 96x96 pixels. Reward is -0.1 every frame and +1000/N for every track tile visited, where N is the total number of tiles in track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points. Episode finishes when all tiles are visited. Some indicators shown at the bottom of the window and the state RGB buffer. From left to right: true speed, four ABS sensors, steering wheel position, gyroscope.\n",
    "\n",
    "<img src=\"./NotebookImages/CarRacing.gif\">\n",
    "\n",
    "Solve this environment with Deep Q-learning. \n",
    "- Skip the first 60 frames of an episode until the zooming has stopped and the car is ready to be controlled.\n",
    "- Crop each state (=image) in such a way that the indicators are removed.\n",
    "- It might be useful to convert the images to grayscale images\n",
    "- You might want to take a couple of consecutive images as one state. \n",
    "\n",
    "The action space can for example look like this:\n",
    "```\n",
    "self.actionSpace = [(-1, 1, 0.2), (0, 1, 0.2),\n",
    "                            (1, 1, 0.2),(-1, 1,0), (0, 1,0),\n",
    "                            (1, 1,0), (-1, 0, 0.2), (0, 0, 0.2),\n",
    "                            (1, 0, 0.2),(-1, 0,0), (0, 0,0), (1, 0,0)]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym-tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
